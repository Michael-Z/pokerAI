{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "\n",
    "1. Finish section on Lasso Regression\n",
    "2. Write model evaluation and validation\n",
    "3. Write justification\n",
    "4. make.sh\n",
    "5. share with group and proof-read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Opponent Modelling in No-Limit Texas Hold'em\n",
    "#### Machine Learning Engineer Nanodegree Capstone Project\n",
    "Nash Taylor, September 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition\n",
    "### Project Overview\n",
    "The problem of creating an artificially-intelligent poker-playing agent is well studied. In fact, some variants of the game such as Limit Hold’em are already considered solved[^1]. However, in more complex variants such as No-Limit Hold’em, there are many factors that limit the effectiveness of such game theoretic models as have been proposed thus far. One of the leading contributors to the difficulty of the game of poker from an AI perspective is its imperfect information; because the hole cards of one’s opponents are not known, predicting their response to different possible actions can prove difficult for even the most experienced of human players. This is the specific sub-problem I will attempt to solve.\n",
    "\n",
    "Predicting a player’s action in a given situation is a perfect example of supervised learning. Given features of the state of the game, the player’s tendencies, the player’s view of his opponents, and the characteristics of the community cards, a supervised model should be able to guess what the player will do next. Note that these features leave out one critical component, which was mentioned earlier: the player’s hole cards. However, as we will see, the combination of a well-engineered feature set and a well-chosen function approximator can make up for this imperfect information state.\n",
    "\n",
    "All supervised models constructed as part of this project utilized data from HandHQ.com[^2].\n",
    "\n",
    "### Problem Statement\n",
    "The goal of this project is to produce a set of supervised learning models that take in features of a poker game and predict the action of whichever player is to act next. The decision of whether to treat this as classification or regression was tricky, and I have settled on a somewhat 'middling' approach: I will do classification for the actions, but for bets and raises I will create a separate regression learner to predict the amounts.\n",
    "\n",
    "Because the rules of the game allow for certain actions in certain situations, and because the state of the board is different after each “street” (dealing of community cards), I have decided to break up the model into 7 different models, one applied to each of the following situations:\n",
    "\n",
    "- Pre-flop, facing a bet\n",
    "- Post-flop, not facing a bet\n",
    "- Post-flop, facing a bet\n",
    "- Post-turn, not facing a bet\n",
    "- Post-turn, facing a bet\n",
    "- Post-river, not facing a bet\n",
    "- Post-river, facing a bet\n",
    "\n",
    "The reason there is no model for “Pre-flop, not facing a bet” is because there are ‘blinds’, which are mandatory bets that begin every game; therefore, there is only one relatively rare situation in which a player would not be facing a bet before the flop, which is in the Big Blind if no bets are made leading up to that player. This is not an interesting prediction problem (they typically check), and there is not enough data to learn this over.\n",
    "\n",
    "The result is 7 models, each taking a slightly different subset of the full feature set and each predicting an action from a subset of the actions described above. In “facing bet” situations, the actions are: fold, call, raise[amount]. In “not facing bet” situations, the actions are: check, bet[amount].\n",
    "\n",
    "The overall procedure for learning these models is as follows:\n",
    "\n",
    "1. Parse the raw text of game logs into a feature set providing information on, for each action taken:\n",
    "    1. the play style of the player (e.g. their preflop raise percentage)\n",
    "    2. the player's opponents at the table (e.g. the average stack size)\n",
    "    3. the community cards (e.g. the number of pairs on the board)\n",
    "    4. the state of the game (e.g. the number of players remaining)\n",
    "2. Split the feature set into 7 subsets, separated by the Round and FacingBet fields. For each resulting dataset, take only the corresponding subset of columns (see Data Preprocessing for full lists)\n",
    "3. For each dataset, train the following:\n",
    "    1. a classification model to predict the action selected by the player (e.g. bet)\n",
    "    2. a regression model to predict the amount of the bet or raise (e.g. 0.55 of pot)\n",
    "4. Evaluate and tune each model using a validation set\n",
    "5. Obtain an overall score over all models to test against the benchmark\n",
    "\n",
    "The result is a set of 14 networks: one for actions, and one for amounts, for each of the 7 situations.\n",
    "\n",
    "### Metrics\n",
    "The variety in the structures of these 14 learning problems necessitates 3 different scoring metrics: for the binary classification of non-bet-facing actions, F1 score; for the multiclass classification of bet-facing actions, multiclass-adjusted F1 score; and for the regression of bet and raise amounts, Mean-Absolute-Error.\n",
    "\n",
    "#### Calculation\n",
    "The F1 computation in code looks like this, where `confusion` is the 2x2 confusion matrix for the binary classification task (whether truly binary or a one-vs-all setting):\n",
    "\n",
    "```\n",
    "precision = confusion[0,0] / confusion[:,0].sum()\n",
    "recall = confusion[0,0] / confusion[0,:].sum()\n",
    "f1 = 2 * (precision*recall) / (precision+recall)\n",
    "```\n",
    "\n",
    "To calculate the F1 for the multi-class case, I issued a custom implementation\n",
    "\n",
    "The mean absolute error is easier represented mathematically, and it is calculated as:\n",
    "\n",
    "$MAE = \\frac{1}{N} \\sum_{i=1}^{N} |p_i - o_i|$, where $p_i$ is the i'th prediction and $o_i$ is the i'th true observation\n",
    "\n",
    "#### Justification\n",
    "\n",
    "The choice of F1 score for binary classification was due to the natural interpretation of the problem as an identification of bets. If the problem is stated as, \"will this person make a bet?\", it becomes obvious what the positive and negative labels are. The classes aren't horribly unbalanced, but they aren't 50/50, so for these reasons F1 score is a good fit.\n",
    "\n",
    "For multiclass, the typical metric is accuracy score. However, because these labels are particularly weighted towards folds (especially in the Preflop case), accuracy score is inadequate for measuring the true predictive value; it considers all labels equal, when really, a model that only predicted folds would do reasonably well, better than random guessing. The best response to this situation is to adapt the F1 score, which is designed for imbalanced classes, to the multiclass setting. This is done by taking the F1 scores of each one-vs-all model (of which we have 3) and taking a weighted average according to the distribution of labels.\n",
    "\n",
    "The main decision for the choice of regression metric is between RMSE and MAE (mean absolute error). When the desired effect is for large errors to be punished linearly proportional to their size, MAE is appropriate. When large errors should be punished with greater effect, RMSE is better. For this problem, making the mistake of assuming a very large raise or bet when it was simply a large raise or bet is not necessarily terrible; the response from the majority of opponents will be fairly robust to this, and so the agent's view of the game and ability to roll forward and plan would not be greatly effected by this mistake. For this reason, I will use MAE instead of RMSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "### Data Exploration\n",
    "The original data collected for this project was a corpus of text files containing logs of online games from 5 different online poker sites. After parsing relevant information from this text, 3 tables were created and a relational database was formed. From these, a large feature set was constructed.\n",
    "\n",
    "#### Boards\n",
    "| Field    | Data Type | Description                                                              | \n",
    "|----------|-----------|--------------------------------------------------------------------------| \n",
    "| GameNum  | String    | Primary key; identifies which game the board is associated with          | \n",
    "| LenBoard | Int       | \"Number of cards on the board (represents round of the game; e.g. Flop)\" | \n",
    "| Board1   | Int       | \"Integer representation of one of the 52 cards in the deck; e.g. 2c = 1\" | \n",
    "| Board2   | Int       | \"Integer representation of one of the 52 cards in the deck; e.g. 2c = 1\" | \n",
    "| Board3   | Int       | \"Integer representation of one of the 52 cards in the deck; e.g. 2c = 1\" | \n",
    "| Board4   | Int       | \"Integer representation of one of the 52 cards in the deck; e.g. 2c = 1\" | \n",
    "| Board5   | Int       | \"Integer representation of one of the 52 cards in the deck; e.g. 2c = 1\" | \n",
    "\n",
    "#### Actions\n",
    "| Field             | Data Type | Description                                                          | \n",
    "|-------------------|-----------|----------------------------------------------------------------------| \n",
    "| GameNum           | String    | Primary key; identifies which game the board is associated with      | \n",
    "| Player            | String    | Obfuscated name of the player                                        | \n",
    "| Action            | String    | Action without amount                                                | \n",
    "| SeatNum           | Int       | Seat number starting from the top right of the table                 | \n",
    "| RelSeatNum        | Int       | Seat number starting from the dealer button                          | \n",
    "| Round             | String    | Round of the game; e.g. Pre-flop                                     | \n",
    "| RoundActionNum    | Int       | \"Numbered actions; reset at the start of each new round (e.g. Flop)\" | \n",
    "| StartStack        | Float     | Amount of chips for Player at the start of the game                  | \n",
    "| CurrentStack      | Float     | Amount of chips for Player at current moment (before action)         | \n",
    "| Amount            | Float     | Amount of chips associated with action                               | \n",
    "| CurrentBet        | Float     | The amount of the bet that Player must respond to                    | \n",
    "| CurrentPot        | Float     | The amount of chips currently at stake                               | \n",
    "| InvestedThisRound | Float     | The amount of chips Player has invested thus far in the round        | \n",
    "| NumPlayersLeft    | Int       | The number of players remaining in the hand                          | \n",
    "| Winnings          | Float     | The amount that Player received at the end of the hand               | \n",
    "| HoleCard1         | Int       | Integer representation of Player’s first hole card                   | \n",
    "| HoleCard2         | Int       | Integer representation of Player’s second hole card                  | \n",
    "| SeatRelDealer     | Int       | Player’s seat number relative to the dealer button                   | \n",
    "| isFold            | Boolean   | Dummy representation of Action                                       | \n",
    "| isCheck           | Boolean   | Dummy representation of Action                                       | \n",
    "| isCall            | Boolean   | Dummy representation of Action                                       | \n",
    "| isBet             | Boolean   | Dummy representation of Action                                       | \n",
    "| isRaise           | Boolean   | Dummy representation of Action                                       | \n",
    "\n",
    "#### Games\n",
    "| Field      | Data Type | Description                                                       | \n",
    "|------------|-----------|-------------------------------------------------------------------| \n",
    "| GameNum    | String    | Primary key; identifies which game the board is associated with   | \n",
    "| Source     | String    | The online poker site from which the game was scraped             | \n",
    "| Date       | DateObj   | The date the game was played                                      | \n",
    "| Time       | DateObj   | The time the game was played                                      | \n",
    "| SmallBlind | Float     | The size of the small blind for that game                         | \n",
    "| BigBlind   | Float     | The size of the big blind for that game (should be 2\\*SmallBlind) | \n",
    "| TableName  | String    | Obfuscated name of the table at which the game was played         | \n",
    "| Dealer     | Int       | Number representing seat number of the dealer button              | \n",
    "| NumPlayers | Int       | Number of players active at the beginning of the hand             | \n",
    "\n",
    "#### Features\n",
    "From these 3 tables, roughly 120 features were produced. To save space, I won’t list them here, but they can be found in the data sample. These features can approximately be broken up into 4 categories:\n",
    "\n",
    "- Features of the play style of Player, e.g. Preflop Raise %\n",
    "- Features of the player’s opponents, e.g. Average Table Stack\n",
    "- Features of the community cards, e.g. Number Of Pairs\n",
    "- Features of the state of the game, e.g. Is Last To Act\n",
    "\n",
    "The majority of these features are numeric, and the breakdown of datatypes is as follows:\n",
    "\n",
    "| Data Type   | Count | \n",
    "|-------------|-------| \n",
    "| Categorical | 1     | \n",
    "| Boolean     | 22    | \n",
    "| Numeric     | 86    | \n",
    "\n",
    "The data being learned over is only a subset of the full data available, due to my own computational limits. This is discussed further in Data Preprocessing. The final shape[^3] of each dataset is:\n",
    "\n",
    "dimTableHere\n",
    "\n",
    "There is a clear decay in the size of the data as we approach later rounds, which makes logical sense; fewer hands get all the way to the river than start at all. The Preflop section is by far the largest and should theoretically be the most effectively trained model.\n",
    "\n",
    "The breakdown of labels for each dataset is:\n",
    "\n",
    "labelTableHere"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Visualization\n",
    "My exploratory visualizations will examine the distributions of actions and amounts, as this is the most important information in the dataset (the label). For the amounts, I will then split the data over rounds. This information was previously presented in table form, but it is always a good idea to view information of this structure in a visual format. Figures 1-3 are, in order: the distribution of actions, the distribution of amounts for bets, and the distribution of amounts for raises. I will discuss these in order.\n",
    "\n",
    "Preflop, it is clear that the majority of actions will be folds. Across future rounds, the distribution between check/bet and fold/call/raise is remarkably similar, with checks consistently making up roughly 60% of actions in non-bet-facing situations, and folds, calls, and raises taking a 50/40/10 split in bet-facing situations. A model that just predicted folds would get about 55% accuracy overall, which is a good benchmark to be aware of.\n",
    "\n",
    "Bet amounts peaked for the most part just over 1/2 of the pot size, which follows from standard poker theory. The distributions are roughly (very roughly) normal, except for the River where there is a secondary mode directly at pot-size. Flop bets had a sharper peak than Turn and River, which tended to be more spread out around the mean.\n",
    "\n",
    "Raise amounts had a distnictly positive skew, as they peaked around 3/4 pot size and decayed from there. The exception was preflop raises. An incredibly distinct pattern emerges in preflop raises, which is that the great majority of players would make 2-pot raises, with strong secondary amounts of 4/3- and 7/3-pot. I would anticipate that this pattern should ease the learning process for this particular network.\n",
    "\n",
    "![Action distribution](data/labelDist.png)\n",
    "\n",
    "![Bet Amount distribution](data/betDist.png)\n",
    "\n",
    "![Raise Amount distribution](data/raiseDist.png)\n",
    "\n",
    "### Algorithms and Techniques\n",
    "Rather than apply a large and complex function approximator like a deep neural network to this problem, I will attempt to find an optimal solution among the simpler and less computationally expensive models. I will conduct this search by thoroughly testing the wide range of classifiers and regressors available in the Python library `sklearn`. The performance of these learners will be measured using cross-validation, to prevent information from the test set from leaking into the training process. Among my primary concerns with the classifiers are:\n",
    "\n",
    "- Training F1 score; does it represent the true function reasonably well?\n",
    "- Testing F1 score; does it generalize?\n",
    "- Prediction time; would a reinforcement learning agent be able to use this to do efficient planning and lookahead?\n",
    "\n",
    "In view of the last point, models with expensive predictions, like the computationally intensive SVM and instance-based (computation deferring) kNN, will not be considered. These are the algorithms I will consider:\n",
    "\n",
    "#### Classifiers\n",
    "##### Decision Tree\n",
    "Decision trees power countless supervised learning algorithms, most of which deploy sets of many trees to generate predictions. The reason decision trees are so commonly used in this way is because they are amazingly fast to train and predict with. A decision tree essentially breaks down the problem of prediction into a series of splits on the features of the training data. At the top of the tree is the feature that splits the data the best according to maximum information gain. At each successive layer of the tree is another set of features, that further splits those groups according to their information gain. This recursive process is applied until the tree reaches some stopping criterion: in the base case, it is when every leaf node contains exactly one kind of label. Because this is a textbook case of overfitting, various hyperparameters can be tuned to reduce the variance of the learner and increase its bias. Such hyperparameters include the maximum depth of the tree, the minimum samples required to split, and the maximum features allowed as splitting criteria. These hyperparameters all control essentially the same characteristic of the tree, its depth, and therefore typically only one is required to be tuned for a given tree.\n",
    "\n",
    "##### AdaBoost\n",
    "AdaBoost builds a collection of simple learners (in my case, decision trees), called an ensemble, and fits an additive model with these trees in an effort to get \"the best of all worlds\". Intuitively, the model is constructed by beginning with a very simple tree, typically with a max depth of 1 or 2, and then successively adding new trees to the model, all of which are trained on the same data. The difference in their training processes is how the data is weighted; if previous learners have been getting examples wrong consistently, then the focus of new learners is shifted towards these examples by increasing their relative weights. This process continues for N steps, fitting N simple (or \"weak\") learners, where N is a hyperparameter to be tuned. The advantage of ensemble methods is that they are unlikely to overfit due to the weak nature of the base classifiers, but they're also unlikely to underfit due to the sample re-weighting and the amount of classifiers present.\n",
    "\n",
    "##### Gradient Boosting\n",
    "Gradient boosting is an adaptation of the AdaBoost algorithm that makes one simple change: where AdaBoost fits successive learners to re-weighted versions of the original data, Gradient Boosting fits to the residuals between the labels and the current predictions. AdaBoost is actually a special case of Gradient Boosting.\n",
    "\n",
    "##### Random Forest\n",
    "Random forests are somewhat the opposite of boosting methods. Where boosting works by fitting a series of *weak* learners to different versions of *all* labels, random forests work by fitting a series of *strong* learners to different *subsets* of the labels. This technique is called \"bagging\", short for bootstrap aggregation. Bootstrapping is the technique whereby one takes random samples of the data with replacement as training sets for each learner; when these strong learners fit to subsets are aggregated, their high variance is mitigated and bias is introduced, thus combating the bias-variance tradeoff in a very effective way. To boost is to start with a high bias learner and add variance; to bag is to start with a high variance learner and add bias.\n",
    "\n",
    "##### Logistic Regression\n",
    "Logistic regression is an extension of linear regression for classification purposes. A linear model is fit using iterative methods (closed form methods such as least squares are not available in this setting due to the residuals not being normally distributed), and that is passed through the logistic function:\n",
    "\n",
    "$\\sigma(t) = \\frac{1}{1+e^t}$\n",
    "\n",
    "which produces a probability that the result is a positive label. Typically, for multi-class, the solver will invoke a one-vs-all method that produces one classifier per label. Logistic regression works great when there is a roughly linear relationship to be exploited, and it is very fast to predict with (plug the values into the equation), although training can take some time.\n",
    "\n",
    "##### Naive Bayes\n",
    "Naive Bayes is a simple application of the basic rules of conditional probability. Using the training data, it computes conditional probabilities for each label given a feature value. The product of all of the conditional probabilities for an example's features for a label is its probability of having that label, and the most likely label is predicted. This model makes a strong assumption about the conditional independence of the features, since their probabilties are simply mulitplied together. However, it has been shown empirically to be successful even when applied to data that does not satisfy this property. This algorithm is fast to train and fast to predict with, and is among the simplest of machine learning classifiers.\n",
    "\n",
    "#### Regressors\n",
    "##### Decision Tree\n",
    "Like a lot of algorithms, decision trees can be extended from classification to regression with a surprisingly simple tweak. Whereas classification trees use their discrete labels to calculate metrics like entropy for measuring a split's effectiveness, regression trees use their continuous labels to calculate metrics like mean squared error. This change in the cost function is really the only thing necessary to extend decision trees from classification to regression; it then continues to split as described before.\n",
    "\n",
    "##### Extra Trees\n",
    "\"Extra\" trees has a somewhat confusing name, as the word \"Extra\" does not mean there are more trees; rather, it is a portmanteau of \"Extremely\" and \"Random\"; it is essentially a random forest, with extra \"random\". This extra randomness comes from the way in which splitting is conducted with respect to the features: the point at which data is split is entirely random. This means that for e.g. a continuous feature with range 0-100, the split could come at any point between 0 and 100. ExtraTrees can be faster to train, but often grow much bigger than the trees in random forests, due to the drop in performance in the splits (random instead of optimal).\n",
    "\n",
    "##### AdaBoost\n",
    "See Regressors --> Decision Tree. The trees in this ensemble are regression trees now, rather than classification trees.\n",
    "\n",
    "##### Random Forest\n",
    "See Regressors --> Decision Tree. The trees in this ensemble are regression trees now, rather than classification trees.\n",
    "\n",
    "##### Linear Regression\n",
    "A classical statistical method that can be derived from many mathematical angles. From the perspective of linear algebra, linear regression is a least squares approximation of the solution to $Ax=b$, where m > n and there is no exact solution; the solution comes from projecting b into the column space of A, which leads to the revised equation $A^TAx=A^Tb$, which has one solution x' and is solvable, since A and b are known. What's interesting about this derivation is that it makes it clear why the least squares solution is optimal: the error vector, $e = b - prj_{C(A)}{b}$, is by definition orthogonal to every vector in C(A), which includes every column of A. When fitting with an intercept, one column of A is the vector of all 1's, which means $e \\cdot [1 ... 1] = \\sum_i{e_i} = 0$. Therefore, the total error of the fit is guaranteed to be 0. Linear regression, as the name suggests, is best used when the relationship is assumed to be linear, as it cannot capture non-linear interactions between features. Linear regression has some other fundamental assumptions, including normally distributed errors, homoscedasticity (similar variance between features and target), and the absence of multicollinearity between features. Linear regression can be trained in closed form if the data matrix is small enough (as it simply involves solving the equation previously mentioned, which can be done without inversion). As the size of the data grows into the millions of records, however, gradient descent may be preferred.\n",
    "\n",
    "##### Ridge Regression\n",
    "Ridge regression adds a term to the sum-of-squared-errors cost function that is equivalent to an L2-regularization on the parameters of the model. The regularization term is added to combat multicollinearity in the data (which, as mentioned, is an assumption of linear regression). There is a Bayesian interpretation for this model, which has its own implementation in sklearn, and both will be explored.\n",
    "\n",
    "##### Lasso Regression\n",
    "While ridge regression is essentially linear regression with L2 regularization, Lasso regression can be thought of as linear regression with L1 regularization. \n",
    "\n",
    "\n",
    "Unsurprisingly, nearly every (non-linear) algorithm is based on decision trees, as they are among the fastest classifiers to train and test with, and speed is a concern. Once a classifier and regressor have been selected, they will be tuned using a grid search approach over their candidate hyperparameters. This grid search optimization will be conducted separately over each of the 7 datasets; the selection of each algorithm, however, will be universal.\n",
    "\n",
    "No feature selection or significant transformation will be performed prior to training, as these features should in theory be designed optimally for the task. Because all of this optimization, from algorithm selection to hyperparameter tuning, will be done using cross-validation, the testing set will be reserved entirely for a final evaluation of the project as a whole. The metrics calculated as part of that evaluation will be over the aggregation of all testing sets across situations; for example, the metric to be compared to the benchmark will be the overall F1 score calculated by summing all of the confusion matrices to provide a single number.\n",
    "\n",
    "### Benchmark\n",
    "For the classification task, the most basic benchmark is the proportion of the most common label, either fold or check. If folds make up, e.g., 60% of labels in one dataset, any classifier should be able to get greater than 60%, otherwise it is no better than a naive model that just predicts folds. For the non-bet-facing datasets, this will be the proportion of checks; for the bet-facing datasets, it will be the proportion of folds. I would anticipate the accuracy of each model to be at least 10% greater than its greatest proportion. As a secondary benchmark, there have been reported accuracy scores using deep networks of 90%[^4], so I would love to be able to beat that (though my metric is F1, not accuracy score, so 90% is a soft value).\n",
    "\n",
    "For the regression case, my focus was centered around a decision I made early in the project design. Originally, my architecture was similar to a paper[^5] I discovered, where the author proposed a single multi-class classification with labels that were essentially binning the bet/raise values at 1/4 pots. To improve on this, I split the problem into classification and regression. If my modification is truly an improvement, I shouldn't have a mean error greater than 1/4 pot, otherwise the classification bins are likely to perform better. For this reason, a mean absolute error of 0.25 is my primary benchmark for regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology\n",
    "### Data Pre-Processing\n",
    "The original data for this project was a corpus of game logs from various online poker sites, such as PokerStars and PartyPoker. These game logs had errors, missing information, names were obfuscated, and because it was from many sources, it had many different formats. The first and most time consuming task of this project was to parse this text into a database of structured data, and then build features from that data.\n",
    "\n",
    "The parsing from text to tables was done in `data/lib/fileReaders.py`, where I wrote 5 separate parsing functions for the 5 data sources, then using Bash and MySQL stored them in a relational database. The database had 3 tables: `actions`, `games`, and `boards`. `games` contained general information for each hand that was played, boards contained the community cards (mapped to ints) for each hand, and actions contained all remaining information for every action in the dataset. This process, for all ~33,000 files, took approximately 5 hours on my local machine. Once the data was parsed, the next step was to construct the feature set, which was done in `data/lib/buildDataset.py`. The result was 114 features covering various aspects of the environment (discussed above in Problem Statement), most of which were specific to certain subsets of the data. Because of the specific nature of these features (which fell out of the inherently different underlying structure of each situation), 7 subsets of the data needed to be separated. This brought the problem from a task of learning one model to learning many, though their structure would remain the same.\n",
    "\n",
    "In terms of feature transformation, only one change was needed, which was to convert LastAction (a string variable) to numeric. This was converted to a series of dummy variables.\n",
    "\n",
    "There were few abnormalities in the data in the sense of illegal or improbable actions. The sites are run with restrictions in place to prevent actions that would result in bad data, and because the stakes of these games were quite high in some cases (as these were games played with real money), the decision-making of the players can be trusted somewhat more than a fake-money site. That said, there were numerous instances in the raw text where information had been overwritten and lost, or errors had been made in recording that resulted in strange values, but for the most part these were cleaned in the orginal processing. Thankfully, the obfuscating of names was consistent, so there was no confusion as to which player was which, even though their real names had been removed.\n",
    "\n",
    "As a final step of pre-processing on the features, I removed all players with less than 50 actions, as their play-style summary statistics would not be accurate and would not be good data to train with. To avoid computational challenges, particularly with fitting the data in-memory, I limited training to the first 1 million rows of each dataset (if it was less than 1 million rows, I used it all).\n",
    "\n",
    "For the labels, the main concern was around all-in plays. These bets and raises were of course associated with abnormally large values, and the question I was faced with was whether to include them in the prediction task. My final decision was ultimately to remove them from the dataset; all-in plays are very rare compared to other actions, and for an agent to predict such a move (which would ultimately end that imagined trajectory and not lead to any more useful data) was something I deemed unnecessary.\n",
    "\n",
    "Overall, the hand-crafting of this feature set straight from raw text was the most intensive of all tasks involved in this project.\n",
    "\n",
    "### Implementation\n",
    "Because the data did not have a hierarchical structure that could be naturally exploited by deep learning, and the features were already hand-crafted, I opted instead for a more traditional supervised learning approach. And rather than settle on one algorithm, I thought it would be best to test as many as possible and compare their performance. Having selected a set of algorithms to test and a set of metrics to compare with, as discussed in Algorithms and Techniques, I first loaded in the dataset I would use to choose the algorithm. Because testing every algorithm on every dataset with cross validation would be too time consuming of a task, I decided to select just one dataset of the 7 and choose an algorithm based on that. I selected the Preflop-True dataset, as it was the largest (and therefore most reliable), but also contained the minimum number of features; if an algorithm can learn effectively with few features, it should be able to learn with many.\n",
    "\n",
    "Before reporting the results, I should note that a special implementation was required in order to obtain an overall F1 score for all classifiers. This was due to their distinct label sets.\n",
    "\n",
    "```\n",
    "import numpy as np\n",
    "fullConfusion = np.zeros((5,5))\n",
    "\n",
    "for df in ['Preflop-True','Flop-False','Flop-True','Turn-False','Turn-True',\n",
    "           'River-False','River-True']:\n",
    "    isFB = df[-4:]=='True'\n",
    "    \n",
    "    #####\n",
    "    ## do grid search and fit optimal learner, get predictions and store in finalPreds\n",
    "    #####\n",
    "    \n",
    "    finalConfusion = confusion_matrix(y_test, finalPreds)\n",
    "    inds = [0,2,4] if isFB else [1,3]\n",
    "    for i,row in zip(inds, finalConfusion): fullConfusion[i,inds] = row\n",
    "\n",
    "def F1fromConfusion(confusion):\n",
    "    allF1scores = []\n",
    "    allProportions = []\n",
    "    for i in range(confusion.shape[0]):\n",
    "        # get 2x2 confusion matrix\n",
    "        newConfusion = np.zeros((2,2))\n",
    "        newConfusion[0,0] = confusion[i,i] # TP\n",
    "        newConfusion[1,0] = confusion[:,i].sum() - confusion[i,i] # FP\n",
    "        newConfusion[0,1] = confusion[i,:].sum() - confusion[i,i] # FN\n",
    "        newConfusion[1,1] = confusion.sum() - newConfusion.sum() # TN\n",
    "        # get F1 from newConfusion\n",
    "        precision = newConfusion[0,0] / newConfusion[:,0].sum()\n",
    "        recall = newConfusion[0,0] / newConfusion[0,:].sum()\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "        allF1scores.append(f1)\n",
    "        # get proportion\n",
    "        prop = confusion[i,:].sum() / confusion.sum()\n",
    "        allProportions.append(prop)\n",
    "        \n",
    "    return np.dot(allF1scores, allProportions)\n",
    "    \n",
    "print \"Final F1 score is:\", F1fromConfusion(fullConfusion)\n",
    "```\n",
    "\n",
    "The idea was to build up a full 5x5 multi-class confusion matrix by updating it with each learner, then manually convert that confusion matrix into a multi-class F1 score. This is how all classifier metrics to follow were obtained. The results were as follows:\n",
    "\n",
    "classifierTableHere\n",
    "\n",
    "Some thoughts on each:\n",
    "\n",
    "- The decision tree seriously overfit, as decision trees do, with an F1 of 1.0 on the training set; it was by far the fastest predictor, though\n",
    "- AdaBoost and Gradient Boosting took the longest to predict by quite a margin (both 3 times that of the 3rd slowest), but their validation scores were the best of the bunch. They are certainly candidates, but the prediction time is a concern that would have to be tuned for\n",
    "- AdaBoost took the longest to predict by quite a margin (3 times that of the 2nd slowest), and its validation score was not good enough to warrant that steep cost\n",
    "- Gradient Boosting performed the best of them all, though it did still overfit (as did all decision tree-based learners); its prediction time was second slowest, but because of its performance it is certainly a contender\n",
    "- Random Forest performed slightly worse than boosting on the validation data, but it did it in one third of the time, which balances out the worse performance. It also overfit quite extremely to the training data, which is likely tunable\n",
    "- Logistic regression is the most interesting one to me. It didn't overfit to the data, but it still produced the second best validation score. Its prediction time was blazingly fast, which makes it a very attractive option\n",
    "- Naive Bayes was one of the fastest predictors, and it actually had better validation accuracy than logistic regression; however, it has no tunable parameters, so I have no ability to improve it like I do with the others\n",
    "\n",
    "The decision came down to validation score versus prediction time; gradient boosting performed the best, and could likely be tuned to overfit less, which would improve the validation score further. But logistic regression wasn't far behind, And it was able to make predictions remarkably fast. In the context of feeding these models to a reinforcement learning agent, that is an important consideration. If logistic regression would allow the agent to perform 15 times more halluncinatory lookaheads for simluation-based planning, and those lookaheads only have 2 more wrong predictions per 100 steps, as the results suggest, the tradeoff is likely beneficial.\n",
    "\n",
    "Selection: **Logistic Regression**\n",
    "\n",
    "Next I compared the regression methods, with these results:\n",
    "\n",
    "regressorTableHere\n",
    "\n",
    "Breaking down each result:\n",
    "\n",
    "- The decision tree once again fit perfectly to the training data, and came out with a below average validation error. Base decision trees are a good baseline to compare to, but they are never a great option when untuned\n",
    "- Extra trees was interesting, because like the decision tree it fit perfectly to the training data, but unlike the decision tree it actually performed the best on the validation data of all the algorithms. Of the ensembles, it was also the fastest to predict. Definitely a strong candidate\n",
    "- AdaBoost was not far behind ExtraTrees in validation accuracy, but where it really suffered was prediction time; it took 3 times longer than any other algorithm to calculate its predictions, which makes it unusable\n",
    "- Random forest performed worse than extra trees, and took slightly longer to predict, which indicates to me that the modification of the extra trees algorithm (the random splits) fits well with this problem and dataset\n",
    "- All of the linear models, with their various regularization techniques, performed horribly on the validation data, in particular base linear regression, which informs me that this is a non-linear relationship and these should not be considered\n",
    "\n",
    "I wasn't overly worried about speed for this model, considering it would be invoked far fewer times than the classifier, but it's nice to see that in this case the fastest ensemble learner was also the best performing. That makes this decision quite easy.\n",
    "\n",
    "Selection: **ExtraTrees Regressor**\n",
    "\n",
    "With the classifier and regressor chosen, I could move on to hyperparameter tuning, which will be discussed in the following section.\n",
    "\n",
    "### Refinement\n",
    "The logistic classifier was tuned with the following dictionary of possible hyperparameter values:\n",
    "\n",
    "| penalty | C   | \n",
    "|---------|-----| \n",
    "| L1      | 100 | \n",
    "| L2      | 10  | \n",
    "|         | 1   | \n",
    "|         | 0.1 | \n",
    "\n",
    "And the best set of hyperparameters found by GridSearchCV, for each dataset, was:\n",
    "\n",
    "classifierGridSearchHere\n",
    "\n",
    "Meanwhile, the regressor was tuned with the following selection of values:\n",
    "\n",
    "| n_estimators | bootstrap | max_features | \n",
    "|--------------|-----------|--------------| \n",
    "| 10           | True      | 30           | \n",
    "| 20           | False     | 50           | \n",
    "| 40           |           | None         | \n",
    "| 80           |           |              | \n",
    "| 200          |           |              | \n",
    "\n",
    "And its best hyperparameters, by dataset, were:\n",
    "\n",
    "regressorGridSearchHere\n",
    "\n",
    "Beyond hyperparameter tuning, not a lot of refinement could be made to these learners. The final results will be discussed next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "### Model Evaluation and Validation\n",
    "The final set of models produced the following F1 scores for each dataset, tested on a held-out testing set that has not been encountered thus far:\n",
    "\n",
    "| Subset       | F1    | \n",
    "|--------------|-------| \n",
    "| Preflop-True | 1.0   | \n",
    "| Flop-False   | 1.0   | \n",
    "| Flop-True    | 1.0   | \n",
    "| Turn-False   | 1.0   | \n",
    "| Turn-True    | 1.0   | \n",
    "| River-False  | 1.0   | \n",
    "| River-True   | 1.0   | \n",
    "\n",
    "And the overall F1 score was ____.\n",
    "\n",
    "The mean absolute error for each test set was:\n",
    "\n",
    "| Subset       | MAE    | \n",
    "|--------------|--------| \n",
    "| Preflop-True | 1.0    | \n",
    "| Flop-False   | 1.0    | \n",
    "| Flop-True    | 1.0    | \n",
    "| Turn-False   | 1.0    | \n",
    "| Turn-True    | 1.0    | \n",
    "| River-False  | 1.0    | \n",
    "| River-True   | 1.0    | \n",
    "\n",
    "And the mean of these is ____.\n",
    "\n",
    "By holding out this data until this point, I have given this data the ability to represent a sample of the real world, which means the results here should accurately describe the model's generalizing ability, and its results can be trusted. ** analyze the results more ** ** do sensitivity analysis ** \n",
    "\n",
    "In this section, the final model and any supporting qualities should be evaluated in detail. It should be clear how the final model was derived and why this model was chosen. In addition, some type of analysis should be used to validate the robustness of this model and its solution, such as manipulating the input data or environment to see how the model’s solution is affected (this is called sensitivity analysis). Questions to ask yourself when writing this section:\n",
    "\n",
    "- Is the final model reasonable and aligning with solution expectations? Are the final parameters of the model appropriate?\n",
    "- Has the final model been tested with various inputs to evaluate whether the model generalizes well to unseen data?\n",
    "- Is the model robust enough for the problem? Do small perturbations (changes) in training data or the input space greatly affect the results?\n",
    "- Can results found from the model be trusted?\n",
    "\n",
    "### Justification\n",
    "\n",
    "In this section, your model’s final solution and its results should be compared to the benchmark you established earlier in the project using some type of statistical analysis. You should also justify whether these results and the solution are significant enough to have solved the problem posed in the project. Questions to ask yourself when writing this section:\n",
    "\n",
    "- Are the final results found stronger than the benchmark result reported earlier?\n",
    "- Have you thoroughly analyzed and discussed the final solution?\n",
    "- Is the final solution significant enough to have solved the problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "### Free-Form Visualization\n",
    "For this visualization, I decided to depict how much computation is required for each stage of this project. From parsing the text files, to building the many kinds of features, to training the classifiers and regressors, each stage is a heavy computational burden, and I think this visualization makes that clear.\n",
    "\n",
    "put data/timings.png here.\n",
    "\n",
    "As you can see, because I limited the amount of text files to about 10%, parsing only took roughly 20 minutes to complete. Had I given it all raw data available, it would have been around 4 hours. Building the dataset is a long, painful process, with many stages, and these are shown here, broken down by the style of computation needed. The features that required me to loop over all rows and hold dictionaries containing certain information were among the slowest, while the simple SQL queries were lightning quick in comparison. Finally, training the classifiers took much longer than training the regressors, due to the amount of data available for regression; only bets and raises were used for those, which was a small subset of the whole dataset.\n",
    "\n",
    "### Reflection\n",
    "This project started when I found a large corpus of text logs of poker games on an old website. I downloaded the data and examined the text files, and found that they were well structured, and contained enough information for me to extract interesting patterns from. This was in contrast to the other poker databases I had found, which had very specific features and in most cases did not even fully describe each action. I knew it would be a challenge to parse this from raw text to a full feature set, and it was, but I felt it was worth it. After writing the original site-specific parsing functions, I used a few quick bash lines to handle the large resulting CSVs, and then imported all the data into a MySQL database for easy retrieval. This turned out to bring a whole new set of challenges as I navigated the world of relational databases, and in some ways it only made things more difficult.\n",
    "\n",
    "From there, I got work building the feature set. Originally, I tried to copy the feature set of related paper, but I found these features weren't performing nearly as well as I wanted. So after spending a few days watching and studying the game of poker, I constructed my own list of features. I did this systematically by breaking the game down into 4 categories: player tendencies, opponent information, game state, and community cards. By developing features for each of these categories, I felt confident that I had represented the information that would be relevant to a professional poker player.\n",
    "\n",
    "Once the data was finally all together, I had the challenge of deciding how to deal with the distinction between action types and action values. Originally, I planned to bin the values of bets and raises and create new labels from these, but I felt that this would lose too much information and would ultimately be less useful for the reinforcement learning agent that these models were designed to be fed to. By considering the situation as two separate problems, I essentially created twice the work for myself in generating models, but my hope was that it would be worth the effort.\n",
    "\n",
    "Actually finding and tuning the classifier and regressor were, as they so often are, the quickest parts of this project. Because of the ease of using Python libraries like Pandas and sklearn, the fitting and predicting process was a breeze. Where I ran into challenges here, however, was that is caused me to realize how many of my features were actually leaking information that the agent was not supposed to know. For instance, when I fit my first decision tree as a classifier on the River-False dataset, it got a near perfect F1 score of 0.999. I knew this was too good to be true, and as it turned out, I had multiple features indicating in some way the amount associated with the action (which, of course, perfectly distinguishes between checks and bets). After a few more of these discoveries I eventually got my feature space to be \"legal\". Ultimately, this project was really a test of my ability to engineer a useful feature set for a complex problem, and the algorithm selection process could be viewed as simply a tool to evaluate that.\n",
    "\n",
    "The final model does about as well as I think I could have hoped. If it were used to inform a poker-playing agent, I feel that the simulation and lookahead it would be able to do would be skirting the line of misleading (as roughly 1 of every 5 predicted actions would be wrong), and that unless the accuracy could return to something above 90%, the agent would be better off going model-free.\n",
    "\n",
    "### Improvement\n",
    "All that said, I feel that the computational limits that were imposed by my local machine seriously limited my ability to learn the problem, and that there are a few very important improvements that could be made.\n",
    "\n",
    "1. Using all of the data. I originally parsed about 270 million rows for the feature set, but time and memory limits prohibited me from using it all. If a learner was fed that amount of data, I have no doubts in its ability to surpass the 90% benchmark I set.\n",
    "2. Deep networks. It's no secret that the combination of deep learning and big data is an effective one, and with the amount of interaction between the many facets of the game, and all that goes into a player's decision-making, I'm sure the architecture of these networks would fit the problem well.\n",
    "\n",
    "### Applications\n",
    "As previously mentioned, this project is not meant to be a standalone model. My original project idea was to build an entire poker-playing AI, using model-based reinforcement learning techniques such as policy gradients to learn the optimal action in every situation. Because poker is a game of imperfect information, certains gaps in the agent's knowledge have to be estimated; these are the play style of the opponents, their hole cards, and the actions they might take in response to its own. By creating a supervised model to predict actions, I have taken the first step and \"solved\" the last task. The problems of guessing hole cards and identifying play style are density estimation and clustering problems respectively, and ones that deserve to be looked at separately. Once these tools have been generated, the agent can begin to learn with its fully constructed MDP, and hopefully converge on a near optimal solution. Of course, it won't be perfect, and it will only be as good as the models it is given, but this is the problem I was faced with in this project. I plan to continue on in this journey, in the hopes of one day releasing the full agent to compete against others and beat the game of No-Limit Texas Hold'em Poker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[^1]: <http://ai.cs.unibas.ch/_files/teaching/fs15/ki/material/ki02-poker.pdf>\n",
    "[^2]: <http://web.archive.org/web/20110205042259/http://www.outflopped.com/questions/286/obfuscated-datamined-hand-histories>\n",
    "[^3]: The actual shapes are slightly larger, as one variable was later converted to a dummy. This would add between 4-6 columns to each dataset.\n",
    "[^4]: https://www.aaai.org/ocs/index.php/WS/AAAIW13/paper/download/7132/6470\n",
    "[^5]: http://www.ai.rug.nl/~mwiering/Tom_van_der_Kleij_Thesis.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
